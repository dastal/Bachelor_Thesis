\chapter{Summary, Conclusions, Limitation \& Future Work} \label{summary}
In this thesis, a malicious DoH detection prototype was implemented into SecGrid. Its components are a feature extraction and a Machine Learning pipeline. The feature extraction component is able to extract informational as well as statistical features out TCP flows that are contained in an input PCAP-file, whereas the ML pipeline is able to detect malicious DoH traffic within two steps using the values that were extracted in the feature extraction. The first step is to separate DoH traffic from normal HTTPS traffic, the second step is to detect malicious DoH traffic.

The modular architecture of SecGrid facilitated the implementation of the two components, therefore no drastic changes of the architecture of SecGrid had to be done. The feature extraction could be implemented as a sub-class of the already available class \textit{AbstractPCAPAnalyser.js}. The \textit{PcapParser} had to be adjusted such that it parses TCP-flows. \textit{node-pcap}, which was previously integrated in the project as a node module had to be forked and changes concerning the handling of non-ending and packet information extraction had to be done. The feature extraction uses the information that is parsed by the \textit{PcapParser} to compute all the needed features.

Further, the two layered Machine Learning detection component was implemented, whereas both Layers use the same algorithm, namely the Light Gradient Boosting Machine Classifier. Both of these models have to be trained with already available data, therefore the data-set CIRA-CIC-DoHBrw-2020 \cite{CIRA-CIC-DoHBrw-2020} was used. This data-set was analyzed with the feature extraction component and two different training data-sets were composed, one for each layer and containing the data that is important for the respective Layer. The ML models of Layer 1 and 2 are trained with the respective data-set and are then ready to predict the input PCAP file. Joining the two components feature extraction and ML model results in the complete prototype for the detection of malicious DoH traffic.

During the implementation, the \textit{TCP\_Tracker} of \textit{node-pcap} had to be adjusted such that it can handle non-ending TCP flows. There it was noticed that there are also other part of the \textit{TCP\_Tracker} that are not completely or not at all implemented, such as the handling of TCP flags different than \textit{ACK} or \textit{FIN}. Further the handling of half-closed TCP flows is not handled, i.e. flows that are ended using only one \textit{FIN/ACK} sequence. As seen in the evaluation of the feature importance, the \textit{state} is an important feature for both, the separation of non-DoH and DoH traffic and the detection of malicious DoH traffic, therefore, by following up the completion of the handling of non-ending TCP flows it is possible to get an even more important feature that helps for the two detection processes.

Another point of the feature extraction module is that when huge amounts of data are tried to be analyzed, it can claim some time. Although it is clear that files that contain some Gigabytes of traffic just need time to be fully analyzed, this process could still be accelerated. This could be accomplished by adapting the \textit{TCP\_Tracker} such that it extracts the data from the PCAP-file in such a way that there is no need for feature extraction to iterate over the same data again. An example therefore is that the \textit{TCP\_Tracker} could compute the time between the first and the current packet or the request- and response time at the point when it retrieves the packets. Currently this is done in the feature extraction, which means that in the whole process it is required to iterate several times over the data and this implicates that feature extraction process is slowed substantially.

The two Layered detection of malicious DoH traffic is extremely accurate while the training and testing data originates from the same data-set. However, as soon as the testing data comes from another data-set than the training data, already the accuracy of the first Layer gets extremely poor. This is probably due to different browser settings and different DoH reslovers that were used to gather data of the two data-sets. Furthermore, both of the used data-sets are lab-generated data-sets, which do not precisely reflect real world data. Another limitation is that only one existing data-set was found that contains non-DoH traffic, benign and malicious DoH traffic, whereas that benign DoH traffic flows are limited to about 30'000 which was just enough to cover the needed flows for the two data-sets. This quantitative lack of data limits the power of the prototype, since the data cannot reflect real world data enough. Since only the first Layer of the prototype could have been tested with other data, it is uncertain that the second Layer is precise in detecting malicious DoH traffic or not. The ML was performed very good with the available data, but there is no evidence for the usefulness of the prototype using real world data. Additionally the malicious DoH traffic was gathered using DoH tunnel tools, but it is also possible the use the DoH protocol without tunnel tools. Thus, this is a further limitation of the prototype and the assumption can be made that the second Layer could also be imprecise when using other data than the data from the data-set \cite{CIRA-CIC-DoHBrw-2020}.

The evaluation of the ML models showed that the prototype basically performs good while the data-sets were analyzed separately, the major problem of the project is the lack of data. In future the data-sets could be further enhanced. This means that the data should not only be lab-generated, but also generated in the real world. Another point is that the detection of malicious DoH traffic could become more precise if the data is gathered only from the communication with one resolver and one browser setting at once, since the evaluation using two different data-sets for training and testing revealed predicting constraints although both the data-sets contain DoH data. Further, to make especially the malicious traffic more reliable, either the data should be generated by readjusting real cyber-attacks or even using data of officially verified cyber-attacks using DoH should be used to bring the data as near to real world scenarios as possible.
